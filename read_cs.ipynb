{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea567bfa",
   "metadata": {},
   "source": [
    "# Reading in citizen science gauge data (1-5 min)\n",
    "\n",
    "### Notes\n",
    "- Intense QC code read in, not sure how easy that will be, I had to clone it off Github and change line 10 of code as got an error with a package, proabbly due to Python versions. Think the package needs updating on Github but not sure who is still in charge of maintaining it. Need ETCCDI data (available in example data for Intense QC)\n",
    "\n",
    "\n",
    "### What does the code do\n",
    "- Reads in Urban Observatory rain gauge data\n",
    "- Reads in National Green Infrastructure Facility rain gauge data\n",
    "- Reads in Acomb Flood Group rain gauge data\n",
    "\n",
    "### What does the code need to do?\n",
    "1. Download the data\n",
    "    - Try API\n",
    "    - Sort out problems with wrong accumulation methods\n",
    "    - Save data\n",
    "2. Quality control data\n",
    "    - Use intense-qc code\n",
    "    - Remove failed gauges\n",
    "    - Remove failed observations\n",
    "    - Save Qc'ed data\n",
    "    \n",
    "### Outputs format\n",
    "- `\\root` folder path \n",
    "    - `\\UO` folder path (1-5 minute rain gauge data)\n",
    "        - `<station-id>_<eastings>_<northings>.csv` - individual (varying resolution) gauge data for station `<station-id>`\n",
    "        - `qc` folder path (quality controlled rain gauge data)\n",
    "            - `<station-id>_<eastings>_<northings>.csv` - individual (15 minute resolution) quality controlled rain gauge data for station `<station-id>`\n",
    "    - `\\NGIF` folder path (1 minute rain gauge data)\n",
    "        - `<station-id>_<eastings>_<northings>.csv` - individual (varying resolution) gauge data for station `<station-id>`\n",
    "        - `qc` folder path (quality controlled rain gauge data)\n",
    "            - `<station-id>_<eastings>_<northings>.csv` - individual (15 minute resolution) quality controlled rain gauge data for station `<station-id>`\n",
    "    - `\\CS` folder path (citizen science rain gauge data)\n",
    "        - `<station-id>_<eastings>_<northings>.csv` - individual (varying resolution) gauge data for station `<station-id>`\n",
    "        - `qc` folder path (quality controlled rain gauge data)\n",
    "            - `<station-id>_<eastings>_<northings>.csv` - individual (15 minute resolution) quality controlled rain gauge data for station `<station-id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cda4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "from datetime import datetime\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8da106",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inputs to change\n",
    "start_date = \"2023-06-20\"\n",
    "end_date = \"2023-06-30\"\n",
    "\n",
    "out_path = r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\PYRAMID\\data\\realtime\"\n",
    "\n",
    "# Bounding box for data \n",
    "e_l, n_l, e_u, n_u = [355000, 534000, 440000, 609000]\n",
    "#lon_l, lat_l, lon_u, lat_u = [-2.6771176, 54.702623, -1.3749203, 55.361917]\n",
    "bbox = [e_l, e_u, n_l, n_u]\n",
    "\n",
    "# Quality control test data path\n",
    "static_data_path = join(out_path, \"static\")\n",
    "intense_path = join(static_data_path, \"intense-qc\")\n",
    "etccdi_data_path = join(join(intense_path, \"tests\"), \"etccdi_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3326c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "uo_outpath = join(out_path, \"UO\")\n",
    "if not exists(uo_outpath):\n",
    "    os.mkdir(uo_outpath)\n",
    "    \n",
    "cs_outpath = join(out_path, \"CS\")\n",
    "if not exists(cs_outpath):\n",
    "    os.mkdir(cs_outpath)\n",
    "    \n",
    "ngif_outpath = join(out_path, \"NGIF\")\n",
    "if not exists(ngif_outpath):\n",
    "    os.mkdir(ngif_outpath)\n",
    "    \n",
    "# new 15 minute timestamp\n",
    "new_timestamp = pd.date_range(\n",
    "    pd.to_datetime(start_date),\n",
    "    pd.to_datetime(end_date) + pd.Timedelta(1, \"d\"),\n",
    "    freq=str(15 * 60) + \"s\", \n",
    "    tz=\"UTC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a3866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 2023-06-23\n",
      "Failed 2023-06-24\n",
      "Failed 2023-06-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 2023-06-26\n",
      "Failed 2023-06-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\Amy\\anaconda3\\envs\\wradlib\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngif.newcastle.ac.uk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### NGIF rain gauge\n",
    "easting = \"424038\"\n",
    "northing = \"564414\"\n",
    "sensor_loc = \"Ensemble E Pit Gauge\"\n",
    "sensor_id = \"Pit rain gauge\"\n",
    "sensor = sensor_loc.replace(\" \", \"-\")\n",
    "\n",
    "# convert timestamp\n",
    "start_time = pd.to_datetime(start_date)\n",
    "end_time = pd.to_datetime(end_date)\n",
    "\n",
    "dates = pd.date_range(start_time, end_time)\n",
    "dates = dates.format(\"%f\")[1:]\n",
    "\n",
    "tabs = []\n",
    "\n",
    "for i in range(len(dates) - 1):\n",
    "    path = \"https://ngif.newcastle.ac.uk/download/Ensemble%20E/Pit%20Rain%20Gauge%23%401m/\" + dates[i] + \"/\" + dates[i + 1]\n",
    "    try:\n",
    "        r = requests.get(path, verify=False)\n",
    "        tab = pd.read_csv(io.StringIO(r.text))\n",
    "        if tab.shape[0] > 0:\n",
    "            tabs.append(tab)\n",
    "    except:\n",
    "        print(\"Failed\", dates[i])\n",
    "        \n",
    "tabs = pd.concat(tabs, ignore_index=True)\n",
    "\n",
    "high_res = 0.2 * pd.Series(tabs[tabs.columns[1]].values, index = pd.to_datetime(tabs.time)) # Stored as tips, 1 is one tip, gauge should have 0.2mm bucket?\n",
    "high_res.index = high_res.index.tz_localize(None)\n",
    "high_res = high_res.sort_index()\n",
    "high_res.to_csv(join(ngif_outpath, sensor + \"_\" + str(easting) + \"_\" + str(northing) + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70066f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Urban Observatory gauge data (including CS Acomb data)\n",
    "\n",
    "# Get list of rainfall stations\n",
    "\n",
    "# convert timestamp\n",
    "start_time = pd.to_datetime(start_date)\n",
    "end_time = pd.to_datetime(end_date)\n",
    "\n",
    "api_date_string_format = \"%Y%m%d%H%M%S\"\n",
    "\n",
    "# get sensors\n",
    "sensor_params = dict(\n",
    "    variable='Rainfall')\n",
    "\n",
    "r = requests.get('http://uoweb3.ncl.ac.uk/api/v1.1/sensors/csv/', sensor_params)\n",
    "sensor_info = pd.read_csv(io.StringIO(r.text))\n",
    "\n",
    "# Function to fix dogdy accumulation that is done by UO maintenence\n",
    "def rescale(data):\n",
    "    rescaled = pd.Series(np.nan, index=data.index)\n",
    "    d = data.dropna()\n",
    "    diffs = d.diff()\n",
    "    cond = (diffs > 0) & (~np.isnan(diffs))\n",
    "    rescaled.loc[diffs[cond].index] = diffs.loc[diffs[cond].index]\n",
    "    return rescaled\n",
    "\n",
    "for sensor in sensor_info[\"Sensor Name\"]:\n",
    "\n",
    "    # get sensor data\n",
    "    data_params = dict(\n",
    "        data_variable='Rainfall',  # variable=Daily%20Accumulation%20Rainfall%2CRainfall\n",
    "        starttime=pd.to_datetime(start_date).strftime(api_date_string_format),\n",
    "        endtime=pd.to_datetime(end_date).strftime(api_date_string_format)\n",
    "    )\n",
    "\n",
    "    path = 'http://uoweb3.ncl.ac.uk/api/v1.1/sensors/{sensor_name}/data/csv/'\n",
    "    path = path.replace(\"{sensor_name}\", sensor)\n",
    "\n",
    "    r = requests.get(path, data_params)\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            data = pd.read_csv(io.StringIO(r.text))\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:27700\")\n",
    "                easting, northing = transformer.transform(data[\"Sensor Centroid Latitude\"].iloc[0],\n",
    "                                                          data[\"Sensor Centroid Longitude\"].iloc[0])\n",
    "                \n",
    "                data_df = pd.Series(data.Value.values, index=pd.to_datetime(data.Timestamp))\n",
    "                rescaled = pd.DataFrame(index=data_df.index)\n",
    "                rescaled[sensor] = np.nan\n",
    "                \n",
    "                if \"ACOMB\" in sensor:\n",
    "                    \n",
    "                    rescaled.loc[: \"2022-05-17\", sensor] = data_df[\"0\"].loc[: \"2022-05-17\"]\n",
    "                    rescaled.loc[\"2022-05-17\" :, sensor] = rescale(data_df[\"0\"].loc[\"2022-05-17\" :])\n",
    "                    rescaled.to_csv(join(cs_outpath, sensor + \"_\" + str(easting) + \"_\" + str(northing) + \".csv\"))\n",
    "                else:\n",
    "                    if \"FS_\" not in sensor:\n",
    "                        rescaled[sensor] = rescale(data_df[\"0\"])\n",
    "                    else:\n",
    "                        rescaled[sensor] = data_df[\"0\"]\n",
    "                    \n",
    "                    rescaled.to_csv(join(uo_outpath, sensor + \"_\" + str(easting) + \"_\" + str(northing) + \".csv\"))\n",
    "        except:\n",
    "            print(\"Not worked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00524e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [DatetimeIndex(['2023-06-20 00:00:00', '2023-06-20 00:15:00',\\n               '2023-06-20 00:30:00', '2023-06-20 00:45:00',\\n               '2023-06-20 01:00:00', '2023-06-20 01:15:00',\\n               '2023-06-20 01:30:00', '2023-06-20 01:45:00',\\n               '2023-06-20 02:00:00', '2023-06-20 02:15:00',\\n               ...\\n               '2023-06-30 21:30:00', '2023-06-30 21:45:00',\\n               '2023-06-30 22:00:00', '2023-06-30 22:15:00',\\n               '2023-06-30 22:30:00', '2023-06-30 22:45:00',\\n               '2023-06-30 23:00:00', '2023-06-30 23:15:00',\\n               '2023-06-30 23:30:00', '2023-06-30 23:45:00'],\\n              dtype='datetime64[ns]', name='time', length=1056, freq='900S')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     tab_15min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m tab\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m15\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m# should be mm/h?\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     filled_gaps \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(np\u001b[38;5;241m.\u001b[39mnan, index\u001b[38;5;241m=\u001b[39mnew_timestamp)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mfilled_gaps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtab_15min\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tab_15min\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(root_filepath)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexing.py:815\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m--> 815\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_setitem_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    818\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexing.py:704\u001b[0m, in \u001b[0;36m_LocationIndexer._get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# GH#45479 test_loc_setitem_range_key\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexing.py:1397\u001b[0m, in \u001b[0;36m_LocIndexer._convert_to_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m key\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1399\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexing.py:1433\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1430\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1431\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1433\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6108\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6106\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6110\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6112\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\wradlib\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6168\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6167\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6170\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [DatetimeIndex(['2023-06-20 00:00:00', '2023-06-20 00:15:00',\\n               '2023-06-20 00:30:00', '2023-06-20 00:45:00',\\n               '2023-06-20 01:00:00', '2023-06-20 01:15:00',\\n               '2023-06-20 01:30:00', '2023-06-20 01:45:00',\\n               '2023-06-20 02:00:00', '2023-06-20 02:15:00',\\n               ...\\n               '2023-06-30 21:30:00', '2023-06-30 21:45:00',\\n               '2023-06-30 22:00:00', '2023-06-30 22:15:00',\\n               '2023-06-30 22:30:00', '2023-06-30 22:45:00',\\n               '2023-06-30 23:00:00', '2023-06-30 23:15:00',\\n               '2023-06-30 23:30:00', '2023-06-30 23:45:00'],\\n              dtype='datetime64[ns]', name='time', length=1056, freq='900S')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "### Accumulate up gauge data\n",
    "\n",
    "for root_filepath in [cs_outpath, ngif_outpath, uo_outpath]:\n",
    "    \n",
    "    outpath_15min = join(root_filepath, \"15min\")\n",
    "    if not exists(outpath_15min):\n",
    "        os.mkdir(outpath_15min)\n",
    "        \n",
    "    for f in os.listdir(root_filepath):\n",
    "        if f.endswith(\".csv\"):\n",
    "            tab = pd.read_csv(join(root_filepath, f), index_col=0)\n",
    "            tab.index = pd.to_datetime(tab.index)\n",
    "            tab_15min = 4 * tab.resample(str(15*60) + \"s\").sum() # should be mm/h?\n",
    "            \n",
    "            filled_gaps = pd.Series(np.nan, index=new_timestamp)\n",
    "            filled_gaps.loc[tab_15min.index] = tab_15min.iloc[:, 0]\n",
    "            \n",
    "            filled_gaps.to_csv(join(outpath_15min, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37000998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2023-06-20 00:00:00', '2023-06-20 00:01:00',\n",
       "               '2023-06-20 00:02:00', '2023-06-20 00:03:00',\n",
       "               '2023-06-20 00:04:00', '2023-06-20 00:05:00',\n",
       "               '2023-06-20 00:06:00', '2023-06-20 00:07:00',\n",
       "               '2023-06-20 00:08:00', '2023-06-20 00:09:00',\n",
       "               ...\n",
       "               '2023-06-30 23:50:00', '2023-06-30 23:51:00',\n",
       "               '2023-06-30 23:52:00', '2023-06-30 23:53:00',\n",
       "               '2023-06-30 23:54:00', '2023-06-30 23:55:00',\n",
       "               '2023-06-30 23:56:00', '2023-06-30 23:57:00',\n",
       "               '2023-06-30 23:58:00', '2023-06-30 23:59:00'],\n",
       "              dtype='datetime64[ns]', name='time', length=7968, freq=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality control gauge data\n",
    "\n",
    "# Import relevent packages\n",
    "os.sys.path.append(r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\Jupyter\\intense-qc\")\n",
    "from intense import gauge, qc, utils\n",
    "from pyproj import Transformer\n",
    "\n",
    "for path in gauge_paths:\n",
    "\n",
    "    qc_output_path = join(split(path)[0], \"qc\")\n",
    "    if not exists(qc_output_path):\n",
    "        os.mkdir(qc_output_path)\n",
    "\n",
    "    output_file = join(qc_output_path, split(path)[1])\n",
    "\n",
    "    data_raw = pd.read_csv(path, index_col=0, parse_dates=True).iloc[:, 0]\n",
    "    data_raw = data_raw.sort_index()\n",
    "    res_min = pd.Series(data_raw.dropna().index).diff().median().seconds / 60\n",
    "\n",
    "    data_15min = data_raw.resample(str(60*15) + \"s\").sum() / 4 # gives mm/h every 15 min\n",
    "    try:\n",
    "        if data.index.dtype == \"datetime64[ns, UTC]\":\n",
    "            data_1h = data_15min.resample(\"1h\").mean()\n",
    "        else:\n",
    "            data_1h = data_15min.resample(\"1h\").mean().tz_localize('UTC')\n",
    "    except:\n",
    "        print(name, \"not read in.\")\n",
    "\n",
    "    station_id, eastings, northings = split(path)[1].split(\".\")[0].split(\"_\")\n",
    "\n",
    "    data = data_1h.dropna()\n",
    "    loc = (eastings, northings)\n",
    "\n",
    "    flags = get_gauge_flags(data, loc)\n",
    "\n",
    "    # only include gauge data that is not flagged\n",
    "    if not flags[\"gauge\"]:\n",
    "        # currenly ignoring year flag as not enough data\n",
    "\n",
    "        # remove flagged observationns\n",
    "        if len(flags[\"obs\"]) > 0:\n",
    "            cond = [idx in flags[\"obs\"] for idx in data_15min.index.round(\"1h\")]\n",
    "            data_15min.loc[cond] = np.nan\n",
    "\n",
    "        data_15min.tz_localize('UTC').to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbca805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
