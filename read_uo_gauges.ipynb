{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea567bfa",
   "metadata": {},
   "source": [
    "# Reading in Urban Observatory rain gauge data (1-5 min)\n",
    "\n",
    "### Notes\n",
    "- Currently the API is down for maintenence so cannot check if this is working.\n",
    "\n",
    "\n",
    "### What does the code do\n",
    "\n",
    "### What does the code need to do?\n",
    "1. Download the data\n",
    "    - Try API\n",
    "    - Sort out problems with wrong accumulation methods\n",
    "    - Save data\n",
    "2. Quality control data\n",
    "    - Use intense-qc code\n",
    "    - Remove failed gauges\n",
    "    - Remove failed observations\n",
    "    - Save Qc'ed data\n",
    "    \n",
    "### Outputs format\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cda4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from os.path import join, exists\n",
    "import os\n",
    "from datetime import datetime\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8da106",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inputs to change\n",
    "start_date = \"2023-06-20\"\n",
    "end_date = \"2023-06-30\"\n",
    "\n",
    "out_path = r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\PYRAMID\\data\\realtime\"\n",
    "\n",
    "# Bounding box for data \n",
    "e_l, n_l, e_u, n_u = [355000, 534000, 440000, 609000]\n",
    "#lon_l, lat_l, lon_u, lat_u = [-2.6771176, 54.702623, -1.3749203, 55.361917]\n",
    "bbox = [e_l, e_u, n_l, n_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e3ea97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uo_outpath = join(out_path, \"UO\")\n",
    "if not exists(uo_outpath):\n",
    "    os.mkdir(uo_outpath)\n",
    "\n",
    "# Get list of rainfall stations\n",
    "\n",
    "# convert timestamp\n",
    "start_time = pd.to_datetime(start_date)\n",
    "end_time = pd.to_datetime(end_date)\n",
    "\n",
    "api_date_string_format = \"%Y%m%d%H%M%S\"\n",
    "\n",
    "# get sensors\n",
    "sensor_params = dict(\n",
    "    variable='Rainfall')\n",
    "\n",
    "r = requests.get('http://uoweb3.ncl.ac.uk/api/v1.1/sensors/csv/', sensor_params)\n",
    "sensor_info = pd.read_csv(io.StringIO(r.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eb1960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa84197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not worked.\n",
      "Not worked.\n",
      "Not worked.\n",
      "Not worked.\n",
      "Not worked.\n",
      "Not worked.\n",
      "Not worked.\n"
     ]
    }
   ],
   "source": [
    "for sensor in sensor_info[\"Sensor Name\"]:\n",
    "\n",
    "    # get sensor data\n",
    "    data_params = dict(\n",
    "        data_variable='Rainfall',  # variable=Daily%20Accumulation%20Rainfall%2CRainfall\n",
    "        starttime=pd.to_datetime(start_date).strftime(api_date_string_format),\n",
    "        endtime=pd.to_datetime(end_date).strftime(api_date_string_format)\n",
    "    )\n",
    "\n",
    "    path = 'http://uoweb3.ncl.ac.uk/api/v1.1/sensors/{sensor_name}/data/csv/'\n",
    "    path = path.replace(\"{sensor_name}\", sensor)\n",
    "\n",
    "    r = requests.get(path, data_params)\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            data = pd.read_csv(io.StringIO(r.text))\n",
    "            if len(data) > 0:\n",
    "                transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:27700\")\n",
    "                easting, northing = transformer.transform(data[\"Sensor Centroid Latitude\"].iloc[0],\n",
    "                                                          data[\"Sensor Centroid Longitude\"].iloc[0])\n",
    "                \n",
    "                data_df = pd.Series(data.Value.values, index=pd.to_datetime(data.Timestamp))\n",
    "                rescaled = pd.DataFrame(index=data_df.index)\n",
    "                rescaled[sensor] = np.nan\n",
    "                \n",
    "                if \"ACOMB\" in sensor:\n",
    "                    \n",
    "                    rescaled.loc[: \"2022-05-17\", sensor] = data_df[\"0\"].loc[: \"2022-05-17\"]\n",
    "                    rescaled.loc[\"2022-05-17\" :, sensor] = rescale(data_df[\"0\"].loc[\"2022-05-17\" :])\n",
    "                \n",
    "                elif \"FS_\" not in sensor:\n",
    "                    rescaled[sensor] = rescale(data_df[\"0\"])\n",
    "                else:\n",
    "                    rescaled[ensor] = data_df[\"0\"]\n",
    "\n",
    "                rescaled.to_csv(join(uo_outpath, sensor + \"_\" + str(easting) + \"_\" + str(northing) + \".csv\"))\n",
    "        except:\n",
    "            print(\"Not worked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ece18c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intense'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# os.sys.path.append(r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\Jupyter\\intense-qc\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gauge, qc, utils\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyproj\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'intense'"
     ]
    }
   ],
   "source": [
    "# Import relevent packages\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "# os.sys.path.append(r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\Jupyter\\intense-qc\")\n",
    "from intense import gauge, qc, utils\n",
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77314ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inputs to change\n",
    "gauge_path = join(out_path, \"UO\")\n",
    "etccdi_data_folder = r\"C:\\Users\\Amy\\OneDrive - Newcastle University (1)\\Documents\\Jupyter\\intense-qc\\tests\\etccdi_data\"\n",
    "\n",
    "gauge_files = [f for f in os.listdir(gauge_path) if f.endswith(\".csv\")]\n",
    "\n",
    "gauges = {}\n",
    "\n",
    "for g in gauge_files:\n",
    "    name = g[0:-4]\n",
    "    data_raw = pd.read_csv(join(gauge_path, g), index_col=0, parse_dates=True).iloc[:, 0]\n",
    "    data_raw = data_raw.sort_index()\n",
    "    res_min = pd.Series(data_raw.dropna().index).diff().median().seconds / 60\n",
    "    \n",
    "    data = data_raw.resample(str(60*15) + \"s\").sum() * 4 # gives mm/h every 15 min\n",
    "    \n",
    "    try:\n",
    "        if data.index.dtype == \"datetime64[ns, UTC]\":\n",
    "            gauges[name] = data.resample(\"1h\").sum()\n",
    "        else:\n",
    "            gauges[name] = data.resample(\"1h\").sum().tz_localize('UTC')\n",
    "    except:\n",
    "        print(name, \"not read in.\")\n",
    "\n",
    "gauges = pd.DataFrame(gauges)\n",
    "gauges = gauges.dropna(axis=0, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gauge_flags(data, loc):\n",
    "    '''Function to apply wokring Intense QC hourly checks on gauge data. \n",
    "    Args: data is the gauge data (pd.Series), loc is the gauge location (eastings, northings)\n",
    "    Returns: flags as a dictionary {gauge flagged, years flagged, obs flagged}\n",
    "    '''\n",
    "    eastings, northings = loc\n",
    "    # convert coordinates to lat/lon\n",
    "    transformer = Transformer.from_crs(\"epsg:27700\", \"epsg:4326\")\n",
    "    latitude, longitude = transformer.transform(eastings, northings)\n",
    "\n",
    "    # create gauge object\n",
    "    rain_gauge = gauge.Gauge(\n",
    "        station_id,\n",
    "        path_to_original_data=\"\",\n",
    "        latitude=latitude,\n",
    "        longitude=longitude,\n",
    "        original_timestep=\"15min\",\n",
    "        original_units=\"mm/h\",\n",
    "        new_units=\"mm/h\",\n",
    "        new_timestep=\"1h\",\n",
    "        data=data\n",
    "    )\n",
    "    rain_gauge.get_info()\n",
    "\n",
    "    # create qc object \n",
    "    test = qc.Qc(\n",
    "        gauge=rain_gauge,\n",
    "        etccdi_data_folder=etccdi_data_folder\n",
    "    )\n",
    "\n",
    "    # checks that don't work on existing data\n",
    "    \"\"\"\n",
    "    test.check_percentiles()\n",
    "    test.check_k_largest()\n",
    "    test.check_intermittency()\n",
    "    test.change_in_min_val_check() \n",
    "    test.cwd_check() # missing function in utils file???\n",
    "    test.change_in_min_val_check() # Change in minimum value check, homogeneity check to see if the resolution of the data has changed. Change flag, flag years\n",
    "    test.find_neighbours(\"hourly\") # frequency: must be either hourly, daily or monthly, Names or names and paths of neighbouring stations\n",
    "    # conditions are: must be within 50km, at least 3 years overlap, select the closest 10, don't have three years of data\n",
    "    # check_hourly_neighbours(), check_daily_neighbours(), check_monthly_neighbours()\n",
    "    test.get_flags() # runs all checks, fails at find_neighbours()\n",
    "    \"\"\"\n",
    "\n",
    "    ### run checks ###\n",
    "\n",
    "    flagged_sdii = 0\n",
    "    sdii_thresh = 100 # just arbitrary atm\n",
    "    if any(np.array(test.get_sdii()) > sdii_thresh): # Simple precipitation intensity index, SDII from ETCCDI and from gauge values (sdii_gridded, sdii_gauge), not sure how to use this\n",
    "        flagged_sdii = 1\n",
    "    \n",
    "    # Flag data if any of these don't return 0:\n",
    "    gauge_checks = [\n",
    "        test.check_days_of_week(), # Checks if proportions of rainfall in each day is significantly different\n",
    "        test.check_break_point(), # Pettitt breakpoint check\n",
    "        flagged_sdii\n",
    "    ]\n",
    "    \n",
    "    flagged_gauge = sum(gauge_checks) != 0\n",
    "    #if flagged_gauge:\n",
    "    #    print(\"Gauge\", np.array([\"days of week\", \"break point\", \"sdii\"])[np.array(gauge_checks) == 1])\n",
    "    #    print(test.check_break_point())\n",
    "\n",
    "    # Flag individual data observations if don't return 0:\n",
    "    obs_checks = pd.DataFrame(index=rain_gauge.data.index)\n",
    "    obs_checks[\"world_record\"] = test.world_record_check_ts() # Checks if and to what degree the world record has been exceeded by each rainfall value, 4, 3, 2 or 1 if exceeded by > 1.5x, 1.33x, 1.22x or 0x respectively and 0 if not exceeded for each value\n",
    "    obs_checks[\"rx1day\"] = test.rx1day_check_ts() # Checks hourly values against maximum 1-day precipitation, Magnitudes of exceedance for each day\n",
    "    obs_checks[\"cdd\"] = test.cdd_check() # ETCCDI provide an index for maximum length of dry spell. Look for suspicious number of consecutive dry hours recorded. Consecutive Dry Days: Maximum length of dry spell, maximum number of consecutive days with RR < 1mm. Magnitudes of exceedence of the length of longest dry period\n",
    "    obs_checks[\"daily_accums\"] = test.daily_accums_check() # Check daily accumulations. Suspect daily accumulations flagged where a recorded rainfall amount at these times is preceded by 23 hours with no rain. A threshold of 2x the mean wet day amount for the corresponding month is applied to increase thechance of identifying accumulated values at the expense of genuine, moderate events\n",
    "    obs_checks[\"monthly_accums\"] = test.monthly_accums_check() # Check monthly accumulations. Flags month prior to high value\n",
    "    obs_checks[\"streaks\"] = test.streaks_check() # Streaks: This is where you see the same value repeated in a run. Currently this records streaks of 2hrs in a row or more over 2 x Monthly mean rainfall. It is considered to be unlikely that you would see even 2 consecutive large rainfall amounts. For this code I have substituted the monthly mean rainfall for SDII as I want the thresholds to be independent of the rainfall time series as the global dataset is of highly variable quality.\n",
    "    flagged_obs = obs_checks.index[obs_checks.sum(1) > 0]\n",
    "\n",
    "    # Flags each individual year:\n",
    "    year_flags = np.array([\n",
    "        test.r99ptot_check_annual(), # Check against R99pTOT: R99pTOT. Annual total PRCP when RR > 99p. Magnitudes of exceedance for yearly 99th percentiles\n",
    "        test.prcptot_check_annual() # check against annual total: PRCPTOT. Annual total precipitation in wet days. Magnitudes of exceedance for yearly totals\n",
    "    ])\n",
    "\n",
    "    flagged_years = np.arange(rain_gauge.data.index.min().year, rain_gauge.data.index.max().year + 1)[year_flags.sum(0) < 0]\n",
    "\n",
    "    return({\"gauge\" : flagged_gauge, \"years\" : flagged_years, \"obs\" : flagged_obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb236de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_gauges = pd.DataFrame(index=gauges.index)\n",
    "for i in range(len(gauges.columns)):\n",
    "    col = gauges.columns[i]\n",
    "    station_id, eastings, northings = col.split(\"_\")\n",
    "\n",
    "    data = gauges.iloc[:, i].dropna()\n",
    "    loc = (eastings, northings)\n",
    "\n",
    "    flags = get_gauge_flags(data, loc)\n",
    "\n",
    "    # only include gauge data that is not flagged\n",
    "    if not flags[\"gauge\"]:\n",
    "\n",
    "        # currenly ignoring year flag as not enough data\n",
    "\n",
    "        # remove flagged observationns\n",
    "        if len(flags[\"obs\"]) < 0:\n",
    "            qc_gauges[col] = gauges[col]\n",
    "        else:\n",
    "            obs_cond = [t not in flags[\"obs\"] for t in gauges.index]\n",
    "            qc_gauges[col] = np.nan\n",
    "            \n",
    "            qc_gauges.loc[obs_cond, col] = gauges.loc[obs_cond, col]\n",
    "\n",
    "# gauges failing on break point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_path = join(gauge_path, \"qc\")\n",
    "if not exists(qc_path):\n",
    "    os.mkdir(qc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af6e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in qc_gauges.columns:\n",
    "    data = qc_gauges[col].dropna()\n",
    "    if len(data) > 0:\n",
    "        data.to_csv(join(qc_path, col + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b4680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
